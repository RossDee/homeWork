#%%


from sklearn import datasets
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error,mean_squared_error
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split,cross_val_score

from sklearn import metrics
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
import numpy as np

#%%
"""
CRIM: åŸé•‡äººå‡çŠ¯ç½ªç‡
ZN: ä½å®…ç”¨åœ°æ‰€å æ¯”ä¾‹
INDUS: åŸé•‡ä¸­éä½å®…ç”¨åœ°æ‰€å æ¯”ä¾‹
CHAS: CHAS è™šæ‹Ÿå˜é‡,ç”¨äºå›å½’åˆ†æ
NOX: ç¯ä¿æŒ‡æ•°
RM: æ¯æ ‹ä½å®…çš„æˆ¿é—´æ•°
AGE: 1940 å¹´ä»¥å‰å»ºæˆçš„è‡ªä½å•ä½çš„æ¯”ä¾‹
DIS: è·ç¦» 5 ä¸ªæ³¢å£«é¡¿çš„å°±ä¸šä¸­å¿ƒçš„åŠ æƒè·ç¦»ã€‚
RAD: è·ç¦»é«˜é€Ÿå…¬è·¯çš„ä¾¿åˆ©æŒ‡æ•°
TAX: æ¯ä¸€ä¸‡ç¾å…ƒçš„ä¸åŠ¨äº§ç¨ç‡
PRTATIO: åŸé•‡ä¸­çš„æ•™å¸ˆå­¦ç”Ÿæ¯”ä¾‹
B: åŸé•‡ä¸­çš„é»‘äººæ¯”ä¾‹
LSTAT: åœ°åŒºä¸­æœ‰å¤šå°‘æˆ¿ä¸œå±äºä½æ”¶å…¥äººç¾¤
MEDV: è‡ªä½æˆ¿å±‹æˆ¿ä»·ä¸­ä½æ•°ï¼ˆä¹Ÿå°±æ˜¯å‡ä»·ï¼‰
"""
boston = datasets.load_boston()
boston.keys()
x = boston.data
target = boston.target
print(x.shape)
print(target.shape)
print(boston.data.shape)
print(boston.feature_names)
print (boston.DESCR)


#%%
"""
1.è®­ç»ƒæ•°æ®:æµ‹è¯•æ•°æ® = 2:8
2.è°ƒç”¨LinearRegression çš„fitæ–¹æ³•
3.è°ƒç”¨LinearRegression çš„ predictæ–¹æ³•
4.mean_squared_error,å‡æ–¹è¯¯å·®
5.mean_absolute_error,å¹³å‡ç»å¯¹è¯¯å·®
6.r2_score:æ‹Ÿåˆä¼˜åº¦
"""
X_train,X_test,y_train,y_test = train_test_split(x,target,test_size=0.20)

model = LinearRegression()
model.fit(X_train,y_train)
y_pred = model.predict(X_test)

print('çº¿æ€§å›å½’ç®—æ³•wå€¼ï¼š', model.coef_)
print('çº¿æ€§å›å½’ç®—æ³•bå€¼: ', model.intercept_)
print("å‡æ–¹è¯¯å·®: %f" % mean_squared_error(y_test,y_pred))
print("å¹³å‡ç»å¯¹è¯¯å·®: %f" % mean_absolute_error(y_test,y_pred))
print("æ‹Ÿåˆä¼˜åº¦: %f" % r2_score(y_test,y_pred))
#print("äº¤å‰éªŒè¯: %f" % cross_val_score(y_test,y_pred))

#%%
"""
ç»˜å›¾

"""
plt.plot(y_pred,y_test,'rx')
plt.plot([y_pred.min(), y_pred.max()], [y_pred.min(), y_pred.max()], 'b-.', lw=4) # f(x)=x
plt.ylabel("Predieted Price")
plt.xlabel("Real Price")

plt.show()

#%%
"""
å‚è€ƒä»£ç ï¼š
https://www.kaggle.com/shreayan98c/boston-house-price-prediction

æ‰‹åŠ¨æ•²å…¥ä»£ç å’Œæ³¨é‡Šï¼Œå¹¶ä¸”å¯¹æ¯ä¸ªå¯¹åº”çš„å‡½æ•°è¿›è¡Œè§£é‡Šï¼Œé¦–æ¬¡çœŸæ­£ä¸Šæ‰‹ä½œä¸šï¼Œç”¨è¿™æ ·çš„ç¬¨åŠæ³•æ¥å­¦ä¹ 

"""
data = pd.DataFrame(x)
data.columns =  boston.feature_names
data.head(5)
#%%
"""
æŸ¥çœ‹æ•°æ®é›†çš„å±æ€§
"""
##adding target varible to dataframe
data['PRICE'] = boston.target
data.shape
data.columns
data.dtypes
## check the unique numbeer of values in dataset
data.nunique()
##check for missing values
data.isnull().sum()
##check the rows with missing values
data[data.isnull().any(axis =1)]
##viewin gthe data statistics
data.describe()
##Finding out the correlation between the features
corr = data.corr()
corr.shape
## plot the heatmap of correlation between features
plt.figure(figsize=(20,20))
sns.heatmap(corr,cbar=True,square=True,fmt='.1f',annot=True,annot_kws={'size':15},cmap='Greens')
plt.show()

#%%
#split target varible and independent variables
## å°†æ•°æ®é›†ä¸­çš„PRICEåˆ—å»é™¤ï¼Œyä¸ºè¾“å‡ºç»“æœ
X = data.drop(['PRICE'],axis=1)
y = data['PRICE']
# Splitting to training and testing data
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 4)



#%%
##Linear regression
"""
Train the model
"""

#Train the model using training sets
lm = LinearRegression()
lm.fit(X_train,y_train)

#%%
"""
coef_:å„ç‰¹å¾çš„ç³»æ•°ï¼ˆé‡è¦æ€§ï¼‰
intercept_:æˆªè·çš„å¤§å°ï¼ˆå¸¸æ•°å€¼ï¼‰
"""
lm.intercept_
## å°†coefficient value è½¬æ¢ä¸ºdafaframe
coeffcients = pd.DataFrame([X_train.columns,lm.coef_]).T
coeffcients = coeffcients.rename(columns = {0: 'Attribute',1:'Coefficients'})
coeffcients


#%%
#Model prediction on train data
"""
ğ‘…^2 : It is a measure of the linear relationship between X and Y. It is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable.

Adjusted ğ‘…^2 :The adjusted R-squared compares the explanatory power of regression models that contain different numbers of predictors.

MAE : It is the mean of the absolute value of the errors. It measures the difference between two continuous variables, here actual and predicted values of y. 

MSE: The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value. 

RMSE: The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value. 
"""

y_pred  = lm.predict(X_train)
#Model Evaulation
print('R^2:',metrics.r2_score(y_train,y_pred))
print('Adjusted R^2:' ,1- (1-metrics.r2_score(y_train,y_pred))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))
print('MAE:',metrics.mean_absolute_error(y_train,y_pred))
print('MSE:',metrics.mean_squared_error(y_train,y_pred))
print('RMSE:',np.sqrt(metrics.mean_squared_error(y_train,y_pred)))

#%%
plt.scatter(y_train,y_pred)
plt.xlabel("Prices")
plt.ylabel("Predicted prices")
plt.title("Prices vs Predicted Prices")
plt.show()

#%%
#check the residuals

plt.scatter(y_pred,y_train - y_pred)
plt.title("Predicted vs residuals")
plt.xlabel("Predicted")
plt.ylabel("Residuals")
plt.show()

#%%
#check the nomality of errors
sns.distplot(y_train - y_pred)
plt.title("Histogram of Residuals")
plt.xlabel("Residuals")
plt.ylabel("Frequency")
plt.show()

#%%
## predicting test data with model
y_test_pred = lm.predict(X_test)

#model Evaluation
acc_linreg = metrics.r2_score(y_test,y_test_pred)
print('R^2:', acc_linreg)
print('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))
print('MAE:',metrics.mean_absolute_error(y_test, y_test_pred))
print('MSE:',metrics.mean_squared_error(y_test, y_test_pred))
print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))